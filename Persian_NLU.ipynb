{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S3F2hMMIJu8"
      },
      "source": [
        "# Translation (FA to EN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB0n87y3LZ2x"
      },
      "outputs": [],
      "source": [
        "f = open('/content/seqfa.txt', 'r')\n",
        "fl = f.readlines()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erVgoTn-HVQL"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFoxuBSuH7da"
      },
      "outputs": [],
      "source": [
        "import googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4s26QXgI5O3"
      },
      "outputs": [],
      "source": [
        "res = []\n",
        "for i in range(len(fl)):\n",
        "  my_text = fl[i]\n",
        "  my_translation = translator.translate(my_text, src='fa', dest='en')\n",
        "  res.append(my_translation.text)\n",
        "  print(i, my_translation.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU3IeZh_Obu1"
      },
      "outputs": [],
      "source": [
        "f_w = open('/content/seqen.txt', 'a')\n",
        "for i in range(len(res)):\n",
        "  f_w.write(res[i]+'\\n')\n",
        "f_w.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hptuzzvIOJe"
      },
      "source": [
        "# Retrieve top-*k* Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSjalZgr7w09"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lkRnb4x7zcY"
      },
      "outputs": [],
      "source": [
        "# === Setup ===\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SentenceTransformer(\"BAAI/bge-large-en\", device=device)\n",
        "\n",
        "# === Load data ===\n",
        "with open(\"/content/train-en.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_samples = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "with open(\"/content/seqen.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_samples = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Lowercased for filtering exact matches\n",
        "train_lc = [s.lower() for s in train_samples]\n",
        "test_lc = [s.lower() for s in test_samples]\n",
        "\n",
        "# === Prepare dense ===\n",
        "def prep_dense(text):\n",
        "    return f\"Represent this sentence for retrieval: {text}\"\n",
        "\n",
        "train_dense_inputs = [prep_dense(t) for t in train_samples]\n",
        "test_dense_inputs = [prep_dense(t) for t in test_samples]\n",
        "\n",
        "# === Encode train set once\n",
        "train_dense = model.encode(train_dense_inputs, normalize_embeddings=True, convert_to_tensor=True, device=device)\n",
        "\n",
        "# === Fit sparse\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_train = vectorizer.fit_transform(train_samples)\n",
        "\n",
        "# === Parameters ===\n",
        "top_k = 5\n",
        "alpha = 0.7  # Dense-TFIDF mix\n",
        "output_path = \"retrieved_alpha0.7_unique.jsonl\"\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for idx, (test_raw, test_lc_text, dense_input) in tqdm(\n",
        "        enumerate(zip(test_samples, test_lc, test_dense_inputs)), total=len(test_samples)\n",
        "    ):\n",
        "        # === Dense\n",
        "        test_dense = model.encode(dense_input, normalize_embeddings=True, convert_to_tensor=True, device=device)\n",
        "        dense_scores = torch.nn.functional.cosine_similarity(test_dense, train_dense).cpu().numpy()\n",
        "\n",
        "        # === Sparse\n",
        "        tfidf_test = vectorizer.transform([test_raw])\n",
        "        sparse_scores = cosine_similarity(tfidf_test, tfidf_train)[0]\n",
        "\n",
        "        # === Hybrid score\n",
        "        hybrid_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n",
        "\n",
        "        # === Filter out exact matches\n",
        "        mask = np.array([train_lc[i] != test_lc_text for i in range(len(train_lc))])\n",
        "        filtered_scores = np.where(mask, hybrid_scores, -np.inf)\n",
        "\n",
        "        # === Sort indices by score\n",
        "        sorted_indices = filtered_scores.argsort()[::-1]\n",
        "\n",
        "        # === Select top-k with UNIQUE text\n",
        "        seen_texts = set()\n",
        "        top_indices = []\n",
        "        for i in sorted_indices:\n",
        "            txt = train_samples[i]\n",
        "            if txt not in seen_texts:\n",
        "                seen_texts.add(txt)\n",
        "                top_indices.append(i)\n",
        "            if len(top_indices) == top_k:\n",
        "                break\n",
        "\n",
        "        retrieved_texts = [train_samples[i] for i in top_indices]\n",
        "\n",
        "        # === Save\n",
        "        fout.write(json.dumps({\n",
        "            \"test_id\": idx,\n",
        "            \"test_en\": test_raw,\n",
        "            \"retrieved_indices\": [int(i) for i in top_indices],\n",
        "            \"retrieved_examples\": retrieved_texts\n",
        "        }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Done. Retrieved top-{top_k} unique examples per test sample → {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sbdnzUvhea7"
      },
      "source": [
        "# Translate Back To Persian (EN to FA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE5GIyAuinyF"
      },
      "outputs": [],
      "source": [
        "import googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u60Uhallhhm_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "# === Load English Examples\n",
        "with open(\"/content/train-en.txt\", encoding=\"utf-8\") as f:\n",
        "    english_examples = [line.strip() for line in f]\n",
        "\n",
        "# === Input and Output JSONL Files\n",
        "input_jsonl = \"/content/retrieved_alpha0.7_unique.jsonl\"\n",
        "output_jsonl = \"translated_examples.jsonl\"\n",
        "\n",
        "resume_from_id = 0  # Replace this with actual test_id where it stopped\n",
        "resume = False  # Will switch to True once we find the starting point\n",
        "started = False\n",
        "\n",
        "with open(input_jsonl, encoding=\"utf-8\") as fin, open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for line in tqdm(fin): #, total=500 # or use sum(1 for _ in open(input_jsonl)) for exact count\n",
        "        data = json.loads(line)\n",
        "        ###\n",
        "        test_id = data[\"test_id\"]\n",
        "        if not resume:\n",
        "            if test_id == resume_from_id:\n",
        "                resume = True  # Found where to resume\n",
        "            else:\n",
        "                continue  # Skip this line\n",
        "\n",
        "        if not started:\n",
        "            print(f\"Resuming from test_id: {test_id}\")\n",
        "            started = True\n",
        "        ###\n",
        "        indices = data[\"retrieved_indices\"]\n",
        "\n",
        "        # Get corresponding English examples\n",
        "        examples_to_translate = [english_examples[i] for i in indices]\n",
        "\n",
        "        # # Translate all at once (batch) for speed\n",
        "        # translations = translator(examples_to_translate, max_length=128)\n",
        "        # translated_texts = [t[\"translation_text\"] for t in translations]\n",
        "\n",
        "        res = []\n",
        "        for i in range(len(examples_to_translate)):\n",
        "          my_text = examples_to_translate[i]\n",
        "          my_translation = translator.translate(my_text, src='en', dest='fa')\n",
        "          res.append(my_translation.text)\n",
        "          print(i, my_translation.text)\n",
        "\n",
        "        # Save new JSONL line\n",
        "        fout.write(json.dumps({\n",
        "            \"test_id\": data[\"test_id\"],\n",
        "            \"translations\": res\n",
        "        }, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afAmXAaRKwL6"
      },
      "source": [
        "# Tokenization (Hazm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyxeCRcYK1XN"
      },
      "outputs": [],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it7k9it7KzZC"
      },
      "outputs": [],
      "source": [
        "from hazm import word_tokenize\n",
        "from hazm import Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwRaZgP9iJAI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyaB3KNHLQ-I"
      },
      "outputs": [],
      "source": [
        "normalizer = Normalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgJ3YR7dKv3l"
      },
      "outputs": [],
      "source": [
        "# === Input and Output JSONL Files\n",
        "input_jsonl = \"/content/translated_examples.jsonl\"\n",
        "output_jsonl = \"/content/tokenized_examples.jsonl\"\n",
        "\n",
        "# === Function to tokenize Persian text using Hazm\n",
        "def tokenize_persian(text):\n",
        "    text = normalizer.normalize(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# === Read the JSONL file, process it, and save tokenized output\n",
        "with open(input_jsonl, encoding=\"utf-8\") as fin, open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for line in tqdm(fin, total=500):  # Adjust based on the number of lines in your input file\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Get the Persian text you need to tokenize (assuming it's in the 'translations' field)\n",
        "        persian_text = data[\"translations\"]\n",
        "        print(persian_text)\n",
        "        # Tokenize the Persian text\n",
        "        tokenized_text = [tokenize_persian(text) for text in persian_text]\n",
        "        print(tokenized_text)\n",
        "\n",
        "        # Save the tokenized data back into JSONL format\n",
        "        fout.write(json.dumps({\n",
        "            \"test_id\": data[\"test_id\"],\n",
        "            \"tokenized_translations\": tokenized_text\n",
        "        }, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLycIweeCaG"
      },
      "source": [
        "# Alignment (SimAlign)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRQWA2cFfkI2"
      },
      "outputs": [],
      "source": [
        "!pip install simalign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j933ENGLgD5j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from simalign import SentenceAligner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-HLk0MKf9_r"
      },
      "outputs": [],
      "source": [
        "# === Load English labels (one per line) ===\n",
        "import ast\n",
        "with open(\"/content/english-atis-tokens-train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    english_tokens = [ast.literal_eval(line.strip()) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glQtRrh_hRV2"
      },
      "outputs": [],
      "source": [
        "# === Load Persian translations ===\n",
        "persian_translations = {}\n",
        "with open(\"/content/tokenized_examples.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        persian_translations[data[\"test_id\"]] = data[\"tokenized_translations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA5dTKkMjEEZ"
      },
      "outputs": [],
      "source": [
        "# === Initialize SimAlign ===\n",
        "from simalign import SentenceAligner\n",
        "aligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C-mGgA5kGv9"
      },
      "outputs": [],
      "source": [
        "# === Prepare output file ===\n",
        "output_file = open(\"aligned_output.jsonl\", \"w\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahhDiDKkkPK2"
      },
      "outputs": [],
      "source": [
        "# === Process main file ===\n",
        "with open(\"/content/retrieved_alpha0.3_unique.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        sample = json.loads(line)\n",
        "        test_id = sample[\"test_id\"]\n",
        "        retrieved_indices = sample[\"retrieved_indices\"]\n",
        "\n",
        "        sample_alignments = []  # list of alignments per retrieved index\n",
        "\n",
        "        for i, idx in enumerate(retrieved_indices):\n",
        "            try:\n",
        "                en_sentence = english_tokens[idx]\n",
        "                fa_tokens = persian_translations[test_id][i]\n",
        "            except (KeyError, IndexError):\n",
        "                print(f\"Skipping index {idx} for test_id {test_id} due to missing data.\")\n",
        "                sample_alignments.append([])  # empty list for alignment\n",
        "                continue\n",
        "\n",
        "            alignment_result = aligner.get_word_aligns(en_sentence, fa_tokens)\n",
        "            # myaligner.get_word_aligns(src_sentence, trg_sentence)\n",
        "            aligned_pairs = alignment_result[\"itermax\"]\n",
        "            sample_alignments.append(aligned_pairs)\n",
        "\n",
        "        # Write the sample output\n",
        "        output_data = {\n",
        "            \"test_id\": test_id,\n",
        "            \"retrieved_indices\": retrieved_indices,\n",
        "            \"alignments\": sample_alignments\n",
        "        }\n",
        "        print(output_data)\n",
        "        output_file.write(json.dumps(output_data, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "output_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym19saIJmNFf"
      },
      "source": [
        "# Slot Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXFSTXQumMvH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load English slot labels (one list per line)\n",
        "with open(\"/content/labels-english-atis-train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    english_slot_labels = [ast.literal_eval(line.strip()) for line in f]\n",
        "\n",
        "# Load Persian translations (tokenized)\n",
        "with open(\"/content/tokenized_examples.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    persian_data = [json.loads(line) for line in f]\n",
        "\n",
        "# Load test file with retrieved indices\n",
        "with open(\"/content/retrieved_alpha0.3_unique.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = [json.loads(line) for line in f]\n",
        "\n",
        "# Load precomputed alignments\n",
        "with open(\"/content/aligned_output.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    alignment_data = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90t6axTJrete"
      },
      "outputs": [],
      "source": [
        "# Collect final output\n",
        "all_mapped_labels = []\n",
        "\n",
        "for test_id, test_sample in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "    retrieved_indices = test_sample[\"retrieved_indices\"]\n",
        "    persian_sentences = persian_data[test_id][\"tokenized_translations\"]\n",
        "    test_alignments = alignment_data[test_id][\"alignments\"]  # list of alignments per retrieved sample\n",
        "\n",
        "    test_labels = []\n",
        "\n",
        "    for i, idx in enumerate(retrieved_indices):\n",
        "        try:\n",
        "            fa_tokens = persian_sentences[i]\n",
        "            en_slot_labels = english_slot_labels[idx]\n",
        "            alignment_pairs = test_alignments[i]\n",
        "        except (IndexError, KeyError):\n",
        "            print(f\"Skipping test_id {test_id}, idx {idx} due to missing data\")\n",
        "            test_labels.append([\"O\"] * len(fa_tokens))\n",
        "            continue\n",
        "\n",
        "        # Initialize all Persian labels as \"O\"\n",
        "        fa_labels = [\"O\"] * len(fa_tokens)\n",
        "\n",
        "        for en_idx, fa_idx in alignment_pairs:\n",
        "            if fa_idx < len(fa_labels) and en_idx < len(en_slot_labels):\n",
        "                fa_labels[fa_idx] = en_slot_labels[en_idx]\n",
        "\n",
        "        test_labels.append(fa_labels)\n",
        "\n",
        "    all_mapped_labels.append(test_labels)\n",
        "\n",
        "# Save output\n",
        "with open(\"mapped_persian_slot_labels.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for label_list in all_mapped_labels:\n",
        "        json.dump(label_list, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(\"✅ Done. Output saved to mapped_persian_slot_labels.jsonl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set API Key"
      ],
      "metadata": {
        "id": "NXFQ8owdDFf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai"
      ],
      "metadata": {
        "id": "dQqg8hu5DD8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import openai\n",
        "openai.api_key = 'your_API_Key'"
      ],
      "metadata": {
        "id": "ZLGz4cNLDBtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spl99zqGyk3X"
      },
      "source": [
        "#SF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqE2hnYNHzGy"
      },
      "source": [
        "## Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR5gxSvZHyz-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def build_prompt_for_sample(test_id, input_tokens, tokenized_dict, mapped_labels_dict, slot_names):\n",
        "    k = 1\n",
        "    examples = tokenized_dict[test_id][:k]\n",
        "    labels = mapped_labels_dict[test_id][:k]\n",
        "\n",
        "    assert len(examples) == len(labels), f\"Mismatch in examples and labels for test_id {test_id}\"\n",
        "\n",
        "    lines = []\n",
        "    # Header\n",
        "    lines.append(\"You are a language model trained to perform slot filling.\\n\")\n",
        "    lines.append(f\"Here are the possible slots: {json.dumps(slot_names, ensure_ascii=False)}\\n\")\n",
        "    lines.append(\"I will provide you with an utterance. Your task is to extract and return the slot values in a structured JSON format using BIO tags for slot filling.\\n\")\n",
        "    lines.append(\"The output structure should be:\")\n",
        "    lines.append(\"   {{\\n       \\\"slots\\\": [\\n           {{\\\"token\\\": \\\"word1\\\", \\\"label\\\": \\\"B-slot_name\\\"}},\\n           {{\\\"token\\\": \\\"word2\\\", \\\"label\\\": \\\"I-slot_name\\\"}},\\n           ...\\n       ]\\n   }}\\n\")\n",
        "\n",
        "    # Few-shot examples\n",
        "    for ex_idx, (tok_list, label_list) in enumerate(zip(examples, labels), 1):\n",
        "        lines.append(f\"Here is one Example:\\n\")\n",
        "        lines.append(f\"    Utterance: {json.dumps(tok_list, ensure_ascii=False)}\")\n",
        "        lines.append(\"    Response:\")\n",
        "        lines.append(\"    {{\\n        \\\"slots\\\": [\")\n",
        "        for tok, lbl in zip(tok_list, label_list):\n",
        "            line = f\"            {{{{\\\"token\\\": \\\"{tok}\\\", \\\"label\\\": \\\"{lbl}\\\"}}}},\"\n",
        "            lines.append(line)\n",
        "        if len(tok_list) > 0:\n",
        "            lines[-1] = lines[-1].rstrip(',')  # remove comma from last line\n",
        "        lines.append(\"        ]\\n    }}\")\n",
        "\n",
        "    # Final utterance\n",
        "    lines.append(\"\\nNow, process this utterance:\\n\")\n",
        "    lines.append(f\"Utterance: {json.dumps(input_tokens, ensure_ascii=False)}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOEjLT64Juwf"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/sampled_utterances.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sampled_utterances = [ast.literal_eval(line.strip()) for line in f]\n",
        "\n",
        "\n",
        "with open(\"/content/tokenized_examples.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    tokenized_dict = {item[\"test_id\"]: item[\"tokenized_translations\"] for item in map(json.loads, f)}\n",
        "\n",
        "\n",
        "with open(\"/content/mapped_persian_slot_labels.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    mapped_labels_dict = {i: labels for i, labels in enumerate(map(json.loads, f))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQuZYnDWYLnZ"
      },
      "outputs": [],
      "source": [
        "# Persian-ATIS\n",
        "slot_names = ['cost_relative',\n",
        " 'class_type',\n",
        " 'toloc.state_code',\n",
        " 'fromloc.city_name',\n",
        " 'toloc.state_name',\n",
        " 'return_date.today_relative',\n",
        " 'fare_amount',\n",
        " 'meal',\n",
        " 'return_date.day_name',\n",
        " 'restriction_code',\n",
        " 'flight_time',\n",
        " 'arrive_time.time',\n",
        " 'depart_time.start_time',\n",
        " 'fromloc.airport_name',\n",
        " 'depart_date.day_number',\n",
        " 'fare_basis_code',\n",
        " 'depart_date.today_relative',\n",
        " 'return_date.date_relative',\n",
        " 'toloc.country_name',\n",
        " 'depart_time.period_mod',\n",
        " 'toloc.airport_name',\n",
        " 'day_name',\n",
        " 'arrive_time.period_of_day',\n",
        " 'toloc.airport_code',\n",
        " 'depart_time.period_of_day',\n",
        " 'days_code',\n",
        " 'return_time.period_of_day',\n",
        " 'arrive_time.end_time',\n",
        " 'time',\n",
        " 'toloc.city_name',\n",
        " 'arrive_time.time_relative',\n",
        " 'state_name',\n",
        " 'depart_date.day_name',\n",
        " 'fromloc.state_code',\n",
        " 'flight_number',\n",
        " 'fromloc.state_name',\n",
        " 'flight',\n",
        " 'arrive_date.day_number',\n",
        " 'depart_time.time',\n",
        " 'airport_name',\n",
        " 'stoploc.state_code',\n",
        " 'depart_time.time_relative',\n",
        " 'meal_description',\n",
        " 'round_trip',\n",
        " 'return_time.period_mod',\n",
        " 'arrive_time.start_time',\n",
        " 'state_code',\n",
        " 'fromloc.airport_code',\n",
        " 'return_date.month_name',\n",
        " 'stoploc.city_name',\n",
        " 'flight_stop',\n",
        " 'airline_name',\n",
        " 'O',\n",
        " 'depart_time.end_time',\n",
        " 'compartment',\n",
        " 'period_of_day',\n",
        " 'transport_type',\n",
        " 'stoploc.airport_name',\n",
        " 'today_relative',\n",
        " 'mod',\n",
        " 'booking_class',\n",
        " 'arrive_date.date_relative',\n",
        " 'airport_code',\n",
        " 'economy',\n",
        " 'meal_code',\n",
        " 'depart_date.date_relative',\n",
        " 'arrive_time.period_mod',\n",
        " 'time_relative',\n",
        " 'arrive_date.today_relative',\n",
        " 'depart_date.year',\n",
        " 'day_number',\n",
        " 'flight_mod',\n",
        " 'depart_date.month_name',\n",
        " 'city_name',\n",
        " 'return_date.day_number',\n",
        " 'arrive_date.day_name',\n",
        " 'airline_code',\n",
        " 'connect',\n",
        " 'or',\n",
        " 'flight_days',\n",
        " 'aircraft_code',\n",
        " 'month_name',\n",
        " 'arrive_date.month_name',\n",
        " 'stoploc.airport_code']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhwQb5BWwVz5"
      },
      "outputs": [],
      "source": [
        "output_file_path = \"Output_SF.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spss3We_K7oh"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "  for test_id, input_tokens in enumerate(sampled_utterances):\n",
        "      messages = [{\"role\": \"system\", \"content\": \"You are a language model trained to perform slot filling.\"}]\n",
        "      prompt = build_prompt_for_sample(test_id, input_tokens, tokenized_dict, mapped_labels_dict, slot_names)\n",
        "      # print(f\"Prompt for test_id {test_id}:\\n\")\n",
        "      # print(prompt)\n",
        "      # print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "      messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "      response = openai.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          messages=messages\n",
        "      )\n",
        "      reply = response.choices[0].message.content\n",
        "      # results.append({\n",
        "      #     \"test_id\": test_id,\n",
        "      #     \"utterance\": input_tokens,\n",
        "      #     \"response\": reply\n",
        "      # })\n",
        "      print(f\"ChatGPT: {reply}\")\n",
        "      print('-' * 60)\n",
        "\n",
        "      # Write the response to the output file\n",
        "      output_file.write(f\"{reply}\\n\")\n",
        "      time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a-QgNacysJX"
      },
      "source": [
        "#ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLAXE5jy-TD3"
      },
      "outputs": [],
      "source": [
        "# Persian-ATIS\n",
        "intent_labels = [\n",
        "'flight_no',\n",
        "'airfare+flight',\n",
        "'capacity',\n",
        "'airfare+flight_time',\n",
        "'quantity',\n",
        "'airfare',\n",
        "'ground_service+ground_fare',\n",
        "'city',\n",
        "'flight_no+airline',\n",
        "'flight',\n",
        "'flight+airfare',\n",
        "'airport',\n",
        "'abbreviation',\n",
        "'cheapest',\n",
        "'aircraft+flight+flight_no',\n",
        "'distance',\n",
        "'restriction',\n",
        "'meal',\n",
        "'aircraft',\n",
        "'flight_time',\n",
        "'flight+airline',\n",
        "'ground_fare',\n",
        "'airline+flight_no',\n",
        "'airline',\n",
        "'ground_service',\n",
        "'day_name'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQqPsSly1rlR"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/sampled_utterances.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sampled_utterances = [ast.literal_eval(line.strip()) for line in f]\n",
        "\n",
        "with open(\"/content/tokenized_examples.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    tokenized_dict = {item[\"test_id\"]: item[\"tokenized_translations\"] for item in map(json.loads, f)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEKVjGMHAiTv"
      },
      "outputs": [],
      "source": [
        "retrieved_indices_list = []\n",
        "\n",
        "with open(\"/content/retrieved_alpha0.7_unique.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        retrieved_indices_list.append(data[\"retrieved_indices\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DqpadFAHGqk"
      },
      "outputs": [],
      "source": [
        "f = open('/content/intent-persian-atis-train.txt', 'r')\n",
        "intent_list = f.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "A6KN9EmJC0C-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m43nbLecyrKN"
      },
      "outputs": [],
      "source": [
        "def build_intent_prompt(test_id, input_tokens, tokenized_dict, retrieved_indices_list, intent_list, intent_labels):\n",
        "    k = 1\n",
        "    examples = tokenized_dict[test_id][:k]\n",
        "    labels = retrieved_indices_list[test_id][:k]\n",
        "    assert len(examples) == len(labels), f\"Mismatch in examples and labels for test_id {test_id}\"\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"You are a language model trained to perform intent detection.\\n\")\n",
        "    lines.append(f\"Here are the possible intents: {json.dumps(intent_labels, ensure_ascii=False)}\\n\")\n",
        "    lines.append(\"I will provide you with an utterance. Your task is to identify the intent of the input text by selecting the most relevant intent from the list.\\n\")\n",
        "\n",
        "    # Few-shot examples\n",
        "    for ex_idx, (tok_list, label_list) in enumerate(zip(examples, labels)):\n",
        "        example_utterance = examples[ex_idx]\n",
        "        example_intent = labels[ex_idx]\n",
        "        lines.append(f\"Example {ex_idx+1}:\\n\")\n",
        "        lines.append(f\"    Utterance: {json.dumps(tok_list, ensure_ascii=False)}\")\n",
        "        # lines.append(f\"    Utterance: {example_utterance}\")\n",
        "        lines.append(f\"    Intent: \\\"{intent_list[example_intent].strip()}\\\"\\n\")\n",
        "\n",
        "    lines.append(\"Now, process this utterance (only write the intent):\\n\")\n",
        "    lines.append(f\"Utterance: {input_tokens}\")\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWq5DpWCCfqX"
      },
      "outputs": [],
      "source": [
        "output_file_path = \"Output_ID.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8DwyFk-Aw6j"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "  for test_id, input_tokens in enumerate(sampled_utterances):\n",
        "      messages = [{\"role\": \"system\", \"content\": \"You are a language model trained to perform slot filling.\"}]\n",
        "      prompt = build_intent_prompt(test_id, input_tokens, tokenized_dict, retrieved_indices_list, intent_list, intent_labels)\n",
        "      # print(f\"Prompt for test_id {test_id}:\\n\")\n",
        "      # print(prompt)\n",
        "      # print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "      messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "      response = openai.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          messages=messages\n",
        "      )\n",
        "      reply = response.choices[0].message.content\n",
        "      # results.append({\n",
        "      #     \"test_id\": test_id,\n",
        "      #     \"utterance\": input_tokens,\n",
        "      #     \"response\": reply\n",
        "      # })\n",
        "      print(f\"ChatGPT: {reply}\")\n",
        "      print('-' * 60)\n",
        "\n",
        "      # Write the response to the output file\n",
        "      output_file.write(f\"{reply}\\n\")\n",
        "      time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "VUtj3ewJtg83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID"
      ],
      "metadata": {
        "id": "-FPZ9CmttlwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
        "\n",
        "acc = accuracy_score(gold_intent, test_intent)\n",
        "\n",
        "averages = [\"micro\", \"macro\", \"weighted\"]\n",
        "metrics = {}\n",
        "\n",
        "for avg in averages:\n",
        "    p, r, f1, _ = precision_recall_fscore_support(gold_intent, test_intent, average=avg, zero_division=0)\n",
        "    metrics[avg] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
        "\n",
        "print(f\"Accuracy: {acc:.3f}\\n\")\n",
        "\n",
        "for avg in averages:\n",
        "    print(f\"{avg.capitalize()} Precision: {metrics[avg]['precision']:.4f}\")\n",
        "    print(f\"{avg.capitalize()} Recall:    {metrics[avg]['recall']:.4f}\")\n",
        "    print(f\"{avg.capitalize()} F1-score:  {metrics[avg]['f1']:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "IH9MskKqtV6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SF"
      ],
      "metadata": {
        "id": "SOigpzUMtfI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to extract labels from JSON strings\n",
        "def extract_labels_from_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Split content based on the appearance of \"json\"\n",
        "    parts = content.split('json')\n",
        "\n",
        "    # Prepare a list to hold the labels\n",
        "    labels_list = []\n",
        "    c = 0\n",
        "    for part in parts:\n",
        "        # Find the first { and last } to capture the JSON object\n",
        "        json_start = part.find('{')\n",
        "        json_end = part.rfind('}')\n",
        "\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            json_str = part[json_start:json_end+1]  # Extract the JSON block\n",
        "\n",
        "            try:\n",
        "                # Load JSON string as a dictionary\n",
        "                data = json.loads(json_str)\n",
        "\n",
        "                # Extract labels\n",
        "                if 'slots' in data:\n",
        "                    labels = [slot['label'] for slot in data['slots']]\n",
        "                    labels_list.append(labels)\n",
        "                    c = c+1\n",
        "                    print(c)\n",
        "                else:\n",
        "                    print(f\"Skipping JSON without 'slots': {json_str}\")\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding JSON: {json_str}\")\n",
        "\n",
        "    return labels_list\n",
        "\n",
        "# Function to write the labels to a file\n",
        "def write_labels_to_file(labels, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for label_list in labels:\n",
        "            # Convert list to string format and write to file\n",
        "            file.write(f\"{label_list}\\n\")\n",
        "\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "Fs3LPa5Gtd60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "sufNH3HotvRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(gold_slot, test_slot))"
      ],
      "metadata": {
        "id": "GqQsxhpfty5F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}